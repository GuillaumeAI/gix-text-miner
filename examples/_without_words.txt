Corpus {
  nDocs: [Getter],
  init: [Function: init],
  addDoc: [Function: addDoc],
  addDocs: [Function: addDocs],
  setAttributes: [Function: setAttributes],
  documents:
   [ Document {
       text:
        'I\' working PhD Goldsmiths, University London Dr Mick Grierson, Dr Rebecca Fiebrink & Prof Huosheng Hu, working title “Realtime image sound synthesis expressive manipulation deep learning reinforcement learning responsive environments” (Quite mouthful I , ’ academia ). My research intersections algorithmic image sound generation + human computer interaction / embodied interaction / expressive gestural interaction + artificial intelligence / machine learning / reinforcement learning / deep learning + responsive environments / virtual reality / augmented reality / mixed reality.\nThe text excerpts literature review September 2015. Now, 4 months I submitted , parts date ( related image/sound synthesis deep learning — developments field incredibly fast!). Also artistic interest field growing rapidly, contributions coming artists academia. So publishing results years , purely academic platforms, I\' publishing case ( bits ) . It aimed academic audience, I\'ve edit compulsory academese, makes sense. It’ meant tutorial ML/DL, overview review current state field ( September 2015), comprehensive review, relation research area. It bit long, read sections make sense, act starting point research.\nIntroduction\nMachine Learning (ML) field Artificial Intelligence (AI) investigates algorithms learn observations data, opposed humans explicitly programming step software . These algorithms enable computers find complex relationships patterns data, produce outputs decisions based statistical models. The field remained primarily academic decades. However recent developments, Deep Learning (DL), increases computing power, ML starting everyday lives. These algorithms pockets powering speech recognition [Hinton al. 2012], email clients filtering spam [Guzella Caminhas 2009], captioning images [Karpathy Fei-Fei 2015], translating text [Sutskever Vinyals 2014] driving cars [Thrun al. 2006].\nFor traditional shallow ML work optimally, high dimensional complex data manually analysed domain specific low-dimensional features hand-crafted[LeCun 2012]. This makes inefficient real world problems. DL algorithms , learn features extract. They eliminate — minimise — manual feature engineering phase learning sequence representations suited problem, allowing operate directly high dimensional, complex, real world data. These qualities made popular recent years, Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN).\nAlgorithmically generating images sound -established research area. However, recent developments field deep learning produced unique results (Nguyen al., 2015; Gatys al., 2015; Mordvintsev al., 2015; Sturm, 2015; Radford al., 2015). Unfortunately, deep learning content generation techniques run realtime interactively. Other recent research combined deep learning agent based AI Monte Carlo Tree Search (MCTS) Reinforcement Learning (RL) adaptive online learning planning. This demonstrated system learning play video games simply ‘watching screen’, prior knowledge game (Mnih al., 2013; Guo al., 2014; Mnih al., 2015).\nFurthermore, converging trends industry gaming, general entertainment media consumption — trend inspired driven primarily anti-disciplinary artists. Major film festivals world Sundance, Tribeca, Toronto exploring promoting ‘interactive storytelling’, ‘nonlinear narrative’ ‘transmedia experiences’. Product launches accompanied compulsory ‘immersive interactive experience’. Similar developments music, dance theatre.\nTechnology confined academia research labs mainstream consumer devices, driving markets. Inspired CAVE- immersive environments [Cruz-Neira al. 1993], 2011 work [Akten al. 2011] consumer projectors Sony PlayStation3’ PSMove controllers projection map living room dynamic content reacting inhabitant. Two years Microsoft Research’ IllumiRoom [Jones al. 2013] RoomAlive [Jones al. 2014] demonstrates interest bringing technology living room. Likewise MIT Media Lab’ SixthSense [Mistry Maes 2009] investigates bringing augmented mixed reality smaller, personal, portable scale. Nintendo’ Wii controller, Sony’ PSMove controller Microsoft’ Kinect depth camera brought alternate interaction paradigms living room. Virtual reality — decades found academia military — making living room Facebook’ recent purchase Oculus Rift, Google’ Cardboard VR, mainstream technology brands. Even Kellogg’ making cereal box cardboard VR headset complete iOS Android app. Within years, launch Microsoft HoloLens Google backed MagicLeap, augmented mixed reality common household experience.\nWe’ve trends hardware platforms, expanding PCs dedicated gaming consoles, mobile phones tablets, potentially platforms emerging smartwatches, Internet Of Things, consumer quadrotor drones, equipped programmable embedded computers, potential platforms augmented games activities.\nThrough pervasive, ubiquitous smart hardware software, developments leading mainstream adoption Multi-Modal Mixed Reality Responsive Environments. The ground-breaking work fields driven -appropriated technology, frequently pioneers discipline bending misuse technology artists creative hackers.\nBrief overview Machine Learning\nMachine Learning field artificial intelligence investigates system improve performance task, respect specific measure, based past experience [Mitchell 1997].\nML algorithms find complex relationships data. They build models based observations, learn rules required make optimum decisions predictions. They recognise patterns data humans recognise. Or recognise patterns, consciously aware formulate program computer traditional -ML ways.\nThis enables create systems exhibit intricate complicated behaviour implement directly. Even behaviour system statistically consistent training data, find patterns unaware , exhibit unexpected behaviour. This quality ML integral research, ability generate behaviour unpredictable, controllable training data. This quality ML greatest strength, greatest danger — introduce biases -fitting -fitting, learn undesired biases found training data. These dangers amplified ML algorithms difficult debug peer inside (Yosinski al., 2015; Caruana al., 2015).\nThe field studied decades. In 1948 essay [Turing 1948], Alan Turing describes machines designed learn named ‘B-type unorganised machines’, conceptual pre-cursors modern day artificial neural networks. For years machine learning remained primarily academic research area. In cases mainstream due high computational requirements inferior performance compared AI methods [LeCun 2014]. However, advances machine learning algorithms, increases computing power — specifically highly paralleled GPU computing — enabled dramatic advancements machine learning applied [Ciresan al. 2011]. Gradually machine learning outperformed AI techniques fields speech recognition [Hinton al. 2012], natural language processing [Collobert al. 2011], computer vision [Couprie al. 2013], email spam filtering [Guzella Caminhas 2009], image captioning [Karpathy Fei-Fei 2015], robotics -driving cars [Thrun al. 2006].\nThe key aspect ML supplying learning algorithm training data. In simple terms, training phase, learning algorithm analyses training data builds model. After training, prediction phase, input data presented model, model processes input produces output, decision prediction.\nThe input model vector ( broadly speaking, tensor, dimensions shape depending problem). The output model vector ( tensor), dimensions input, depending problem.\nThe model parameterised model parameters, ’ model parameters learning algorithm learn. Training model consists Objective function ( Loss, Cost Energy function) learning algorithm find set model parameters minimise Objective function training data ( Utility function algorithm maximise).\nIn reality, ML implementations lot complicated , clear distinctions training prediction phases (.. Online Learning, Transfer Learning, Reinforcement Learning model updated continually data ), concepts root .\nSupervised Learning\nIn Supervised Learning, model trained labelled data, training input-target pair. During training, learning algorithm learn model parameters effectively implement function maps input training pair, target. These targets thought supervisory signal. For classification problems, target discrete class label, represented -hot vector (.. vector elements , entry desired class, ). For regression problems, target real-valued vector ( tensor). Having input-target pairs makes straight forward objective function, Supervised Learning popular successful branches ML. However, training pairs manually people. This makes cumbersome time consuming prepare. Online crowd-sourcing platforms Mechanical Turk helped accelerate preparation large labelled datasets ’ starting success field (LeCun, 2014).\nUnsupervised Learning\nIn Unsupervised Learning, training performed unlabelled data. Without external supervisory signal, ambiguous Objective function, unsupervised learning big open problems ML (Le al. 2012). One common training objectives unsupervised learning found Auto-Encoder, target input, learning algorithm learn compress decompress training minimal loss. If successful, finds regularities training data, learns compact meaningful representations. Another common objective clustering, learning algorithm organise training data groups based similarities learn. When inputs presented model trained unsupervised learning methods, model transform input data compact representations, predict relates data based patterns found.\nSemi-supervised Learning\nA combination — Semi-supervised Learning— data manually labelled, isn’. Upon training, model learns classify data, associate unlabelled items correct labels supplied training.\nReinforcement Learning\nIn Reinforcement Learning (RL), model trained labels associating inputs targets. It supervised ( direct supervisory signal) unsupervised ( supervisory signal), delayed reward signal (Kaelbling, Littman, & Moore, 1996). The terminology slightly RL, decisions predictions called actions, decision making ( action taking) entity called agent. This RL based Markov Decision Process (MDP) (Bellman, 1957), notion time, time-step, agents actions move states. In RL, time-step, agent receives reward environment. But reward reward action, delayed reward action series actions earlier , related ‘random’ events agent’ control knowledge. Part RL solve attribution problem delayed rewards. The general objective algorithm learn optimal decisions maximising long term reward. This process involves balance exploration ( actions haven’ made) exploitation ( actions reward higher ). RL thought learning trial error.\nDeep Learning motivations\nDeep Learning (DL) family architectures methods aims minimise eliminate domain-specific feature engineering learning sequence -linear feature transformations hierarchical representations (LeCun, 2014).\nTraditional machine learning techniques — support vector machines shallow neural networks — unable work highly complex, high dimensional data. When working models, pre-process data, reduce dimensions extract hand-crafted, domain-specific features, process called feature engineering [LeCun 2012]. Collectively, features called representation ( data). The learning algorithm trained hand-crafted representation. The process feature engineering difficult, time-consuming requires skill [Ng 2013]. Furthermore, success training highly dependent chosen representation [Bengio al. 2013]. As result, feature engineering approaches provide inconsistent, unreliable results.\nUsing deep learning techniques, pre-processing feature extraction steps skipped. Instead, model fed higher dimensional, raw data. The deep learning model stack parameterised, -linear feature transformations learn hierarchical representations [LeCun 2014]. During training, layer learns kinds transformations apply previous layer, .. kinds features extract . As result, deep learning model hierarchy representations increasing level abstraction.\nThis makes deep learning powerful handling real world data. However, model parameters learn, millions, cost complex implementations, higher computational requirements, larger training sets [LeCun al. 1998].\nIn summary: A machine learning model requires representation world. In shallow learning, representation hand crafted domain-specific pre-processing feature extraction. In deep learning, raw data directly fed deep model algorithms learn hierarchy representations.\nHowever, deep learning domain-specific feature engineering , representation input data important. A key component implementing successful deep learning system, relationship representation architecture model, compatible relevant features extracted efficiently inputs [Bengio al. 2013]. In seminal research [Krizhevsky al. 2012], authors found removing convolutional layers — contained 1% 60 million parameters — resulted inferior performance.\nDL researchers aware limitations architecture design active research area. Thus DL silver bullet / universal learning algorithm media hype proposes .\nVery history Deep Learning\nAn -depth survey DL related algorithms scope text ’ll focus recent major milestones relevant research. For -depth survey (Schmidhuber, 2015).\nIn [LeCun al. 1989] Yann LeCun Convolutional Neural Network (CNN) — deep learning neural network layers connections inspired found biological systems — recognise handwritten digits. However computers era powerful run operations required network, additional DSP board needed. Over twenty years LeCun’ research showed CNNs ability learn recognise patterns image speech recognition [LeCun Bengio 1995][LeCun al. 1995][LeCun al. 1998][LeCun al. 2004].\nHowever, DL algorithms require great deal computing power. Especially dealing large inputs images, case amount computation scales linearly number pixels [Mnih al. 2014]. CNNs practical real world applications highly parallel Graphical Processing Units (GPU) [Raina al. 2009]. This led explosion DL implementations. Quite famously, Quoc Le al developed software learned detect faces cats extract features sampling random frames 10 million YouTube videos [Le al. 2011].\nWith parameters train, CNNs require massive datasets. In absence data, CNNs outperformed older, handcrafted, specialist pattern recognition algorithms. With introduction large datasets millions labelled images thousands categories, ImageNet [Deng al. 2009], balance shifted. In 2012 Geoffrey Hinton’ students Alex Krizhevsky Ilya Sutskever designed deep CNN architecture 60 million parameters trained GPUs [Krizhevsky al. 2012]. Their model outperformed traditional image recognition methods large margin. This turning point led improvements recent years, dramatically decreasing errors predictions.\nA similar pattern adoption Recurrent Neural Networks (RNNs) — deep neural networks cyclic connections, store internal states, allowing process sequential, temporal data. In field speech recognition, hand-crafted Gaussian Mixture Model — Hidden Markov Model approaches consistently outperforming methods, including DL. Once large datasets computing power , RNNs started outperform GMM-HMM widespread speech recognition [Hinton al. 2012][Deng al. 2013].\nAs , significant aspect successful application DL involves recognition complex patterns.\nRecent research demonstrated deep learning approaches classification recognition tasks. In [Ilya Sutskever, Oriol Vinyals 2014], authors DL translate text. This involved considerable challenge dealing input output sequences variable length. The authors Long Short-Term Memory (LSTM) [Hochreiter Schmidhuber 1997] RNN, approach outperform previously methods solving problems. In [Graves 2013], Alex Graves demonstrated LSTM networks generate variety sequential outputs including long chunks text hand-writing. Further research field shown promise respect deep learning artistic, creative output.\nDeep Learning Artistic & Creative Output\n(NB: section date 10x longer!)\nIn [Erhan al. 2009], authors curious methods qualitative analysis network architectures, effects varying inputs specific neuron activity hidden layers deep models. They achieved gradient ascent image classification deep models. With method generate inputs maximised neuron activity layers interest Stacked Denoising Autoencoders Deep Belief Networks.\nIn 2013 Researchers Oxford University [Simonyan al. 2013] similar gradient ascent method generate images maximise class score convolutional network trained ImageNet. They similar technique generate class saliency map, input image class.\nThe year, Oxford University researchers [Mahendran Vedaldi 2014] frustrated lack knowledge internals image representation convolutional networks trained image classification. They applied gradient ascent ‘invert’ CNN, reconstructing images hidden layers neurons. They found CNN trained images, ImageNet, stored photographic information layers, abstract features edges, shapes gradients. The visual outputs methods include abstract — recognisable — representations trained images classes.\nIn 2015, Alexey Dosovitskiy Thomas Brox devised method ‘inverting’ CNNs’ visualise internal representations convolutional neural network convolutional network [Dosovitskiy Brox 2015].\nAlso 2015, Anh Nguyen al curious recognition deep image classification models differed visual recognition humans. They convolutional networks trained ImageNet MNIST handwriting dataset [LeCun al.], combined evolutionary algorithms gradient ascent generate images scored highly specific classes, unrecognisable humans [Nguyen al. 2015]. They found cases generate images satisfy classes 99.99% accuracy CNN, completely unrecognisable humans (.. detecting cheetah white noise, starfish wavy lines .). Interestingly, submitted output images art contest, 21.3% submitted artworks selected exhibition.\nAgain 2015 Google researchers [Mordvintsev al. 2015] released code research called #DeepDream #Inceptionism, viral social media. They similar gradient ascent generate images maximised activity hidden neurons, fed generated output back input create feedback loops acted amplify activity. Combined image transformations iteration, created endless fractal- animations ‘hallucinations’ abstract — subtly recognisable — imagery. They GoogLeNet convolutional network architecture research, details found [Szegedy al. 2014].\nAlso 2015 [Gatys al. 2015] released similar research highly shared social networks, called #StyleNet. This research extracts artistic style image — , painting Van Gogh Edvard Munch, applies image, photograph. Techniques applying artistic styles images researched years — subset nonphotorealistic rendering, 2013 survey found [Kyprianidis al. 2013]. However cases algorithms hand-crafted resemble style ( exception [Mital al. 2013]). In Gatys al.’ research, authors found convolutional neural networks separate content style image, storing representations . Doing enabled apply transformations, mix match representations images — .. applying style Van Gogh’ ‘Starry Night’ photograph. This technique works remarkably abstract styles Mark Rothko, Jackson Pollock, Piet Mondrian.\nSimilar developments place sequenced data recurrent neural networks. In 2015 Andrej Karpathy released open-source Recurrent Neural Network implementation training character level language models called char-rnn [Karpathy 2015a] based [Graves 2013]. The software takes single text file, generates similar text based character sequence probabilities. This software number people generate text style Shakespeare, cooking recipes, rap lyrics, Obama speeches, bible [Karpathy 2015b]. This char-rnn library [Sturm 2015] generate midi notes style folk music. Due simple implementation text sequence, limited monophonic output. Other examples composing music recurrent networks include [Eck Schmidhuber 2002][Boulanger-Lewandowski al. 2012].\nIn addition, 2013, DeepMind technologies (recently acquired Google) developed system capable learning play Atari games simply observing images screen [Mnih al. 2013]. Given prior knowledge game rules controls, screen pixels input, system developed strategies days playing. In cases strategies outperformed AIs, cases outperformed human players. They achieved implementing deep reinforcement learning algorithm training convolutional networks Q-learning, approach call Deep Q Networks (DQN). In recent research [Guo al. 2014], authors investigated methods improving DQN’ performance Monte Carlo Tree Search methods [Browne Powley 2012]. Not access internal game-state, researchers train CNN offline UCT screen pixels input, trained CNN runtime policy select actions. They found UCT trained CNN state art realtime AI beat score DQN. However order achieve results long offline training period.\nThese recent developments deep learning networks generate images, sounds actions show incredible potential application deep learning creative output. There unexplored territories , performance realtime, interactive.\nBrief History Algorithmic Computational Art ( relation ML/AI)\nIn section I slight digression acknowledge algorithmic art prior Deep Learning. An indepth survey scope review, serves summary area. A comprehensive review found [Grierson 2005] [Levin 2000].\nThe computers purposes making art dates back 1950s 1960s, notably John Whitney’ DIY analog computers built World War II M5 M7 targeting computers anti-aircraft machinery [Alves 2005]. Whitney’ works pioneering technically, leading birth computer graphics special effects scenes Stanley Kubrick’ 2001: A Space Odyssey, addition pioneered field computer-aided audio visual composition [Grierson 2005]. His work continues tradition experimental abstract animators filmmakers Normal McLaren Oskar Fischinger. He joined shortly software artists Paul Brown, Vera Molnar, Manfred Mohr, Frieder Nake, Larry Cuba .\nHowever Harold Cohen’ AARON software 1973 [Cohen 1973] introduced artificial intelligence computer art. AARON ostensibly piece software, written understand colour form. Cohen talks ‘training’ software [Cohen 1994]. However term rhetorically. The ‘learning’ AARON machine learning mentioned previous sections. AARON learn data. Instead, Cohen AARON learn , analyse , implement sets rules required replicate behaviour. Often complex sets rules years Cohen learn program [Cohen 2006].\nOther computer graphics artists working artificial intelligence shortly include William Latham [Todd Latham 1992], Karl Sims [Sims 1994], Scott Draves [Draves 2005]. Inspired Darwinian evolution natural selection, artists primarily explored Evolutionary Algorithms (EA) — Genetic Algorithms (GA) — creation algorithmic art, eventually evolutionary art. Also period, David Cope developed algorithm composing music. His ‘Experiments Musical Intelligence (EMI)’ began 1981, developed decades eventually patented algorithm ‘Recombinant music composition algorithm’ [Cope 2010]. Using algorithm generated musical sequences style classical composers styles, Bach, Vivaldi, Beethoven, Mozart, Chopin Debussy. His latest software ‘Emily Howell’ albums released .\nStarting 1960s Myron Kruger developed gesturally interactive computer artworks Responsive Environments, culminating seminal Artificial Reality environment ‘Videoplace’ [Krueger al. 1985]. Videoplace tracked users cameras, enabling interact virtual objects scene projectors. First developed 1986, David Rokeby’ ‘Very Nervous System’ explores similar themes gestural full body interaction — hand built cameras — case generate music [Rokeby 1986]. Other notable artists working similar ideas era include Ed Tannenbaum, Scott Snibbe, Michael Naimark, Golan Levin Camille Utterback.\nWith introduction creative coding tools open-source communities exponential growth field decades. Some tools global communities include Processing, openFrameworks, Cinder, vvvv, Max/MSP/Jitter, PureData, SuperCollider, TouchDesigner, QuartzComposer, Three.js smaller bespoke .\nThis process driven creative art form traced back wider rule-based generative art movement includes composers Steve Reich, John Cage, Terry Riley, Brian Eno artists Sol Lewitt Nam June Paik.\nComputational Creativity\nA -field AI related area Computational Creativity. Whereas AI questions machine , exhibit intelligent behaviour [Turing 1950], Computational Creativity questions machine creative, exhibit creative behaviour.\nComputational Creativity research concerned creative output algorithms technical implementation details, equally — — concerned philosophical, cognitive, psychological semantic connotations machines exhibiting creative behaviour, acting creative. In [McCormack ’Inverno 2012] related papers [McCormack D’Inverno 2014], authors — attempt answer — questions computers creativity, creative agency role creative tools.\nAs part philosophical angle computational creativity research, emphasis fully autonomous systems. The field includes research software exhibits intentionality, justify decisions makes creating piece work framing information context work [Colton Wiggins 2012]. This thought analogous artist making deliberate, purposeful decisions step creative process. This aspect computational creativity referred strong computational creativity [Al-rifaie Bishop 2015] — analogous John Searle’ strong ( weak) AI [Searle 1980]. The field accompanied formalisms, proposed models theories creativity ensure systems’ behaviour complies thought ‘creative behaviour’ [Colton al. 2011].\nWithin context research systems conceive fictional concepts [Cavallo al. 2013], design video games [Cook al. 2014], write poetry [Colton al. 2012], inherently ‘creative’ tasks.\nNB: While algorithmic techniques content generation Computational Creativity scope research, formalisms models creativity . It research interested weak computational creativity — semi-autonomous, collaborative creativity human interaction content creation process relevant, essential — create interactive systems human user guide computationally creative system realtime.\nMachine Learning Artistic, Expressive Human Computer Interaction (AEHCI)\nIntroduction\nIn previous sections I reviewed -exhaustive range relevant literature areas algorithmic image sound generation, ranging simple algorithms latest developments deep learning. Some -realtime, content generated application deep learning, realtime, interactive. This section cover Artistic Expressive Human Computer Interaction (AEHCI) — Human Computer Interaction artistic expression.\nIn [Dourish 2001] Paul Dourish proposes models interactive system design. Embodied Interaction interaction embodied environment, physically, fundamental component setting. It interaction design takes consideration ways experience everyday world. This philosophy applicable designing gestural interfaces artistic expression.\nAs mentioned , Myron Kruger interested exploring Responsive Environments, ‘interaction central, peripheral issue’ [Krueger al. 1985]. He potential area arts, education, telecommunications general human-machine interaction motivated creating playful environments explore perceptual process navigate physical world.\nHuman computer interaction music — Musician-Computer Interaction (MCI) [Gillian 2011] — academically established fields related AEHCI, gestural human computer interaction visual composition. However, requirements interaction design gesture recognition similar. Both require low-latency, realtime systems configured --fly. They capable detecting wide range gestures, AEHCI systems concentrate subtle finger movements, track bodies multiple people. Furthermore, ability detect respond subtle variations gestures essential convey expressivity [Caramiaux al. 2014]. Also, performance situations, gesture recognition generalized people, training specific performing individual maximise personal expression [Gillian 2011].\nDue similarities, research Musician-Computer Interaction base model expressive gestural interaction, built general AEHCI.\nGestures ‘Expressive Gesture’\nA survey definitions gesture, relation music found [Cadoz Wanderley, 2000]. The authors conclude proposed definitions adapt gesture music, purposefully avoid providing definitions, focusing aspects definitions apply.\nIn [Camurri al., 2004] authors define Expressive Gesture “responsible [sic] communication information call expressive content” “Expressive content concerns aspects related feelings, moods, affect, intensity emotional experience”. This definition Expressive Gesture research, complemented “natural, spontaneous gestures made person telling story”, [Cassell Mcneill, 1991]. Particularly semiotic classification metaphoric, indicating abstract ideas [McNeill Levy, 1980]. A wider study gesture expressivity dimensions — context musical performance human computer interaction found [Caramiaux, 2015].\nThis research concerned detecting emotion gesture [Cowie al., 2001] [Zeng al., 2009]. Instead concerned finding correlations parameters gesture, parameters generative output model. It map expressive gesture trained models artistic content synthesis manipulation. Inspired research embodied cognition relationship action perception [Kohler al., 2002, Metzinger Gallese, 2003, Leman, 2007], [Caramiaux al., 2009] authors investigate similar relationships analysing motion capture data participants performing free hand movements listening short sound samples.\nGesture Recognition Sensors\nOne significant challenges executing gestural interaction reading relevant information user — recognising positions, movements gestures. A challenge extrapolating motivations intentions gestures.\nThere hardware devices sensors support process: accelerometers inertial measurement units (IMU), myoelectric sensors, ultrasonic infra-red range finders, 2d cameras / depth cameras / computer vision (CV), radar, lidar . Surveys gesture recognition technology research found [Gillian 2011] [LaViola Jr. 2013].\nMy research investigate modes sensing. It focuses emerging consumer technology, applications algorithmic image sound synthesis manipulation context. This remain applicable hardware environments relate commercial gaming mainstream . Primarily involve depth cameras similar Microsoft’ Kinect 2 LeapMotion, computer vision traditional 2D cameras.\nFurthermore, increasing trend consumer devices combine multiple sensors varied data, contributing higher accuracy estimations pose movement. Past examples include Nintendo’ Wiimote controller combining accelerometer infrared sensor ( additional gyroscope MotionPlus addon ), Sony’ PSMove controller combining 6-axis IMU magnetometer high speed computer vision. Both controllers feature buttons D-pad vibration based haptic feedback. Microsoft’ Kinect combines RGB camera, IR camera / depth sensor, microphone array, accelerometer ( detect device orientation) single, affordable consumer device.\nNext generation devices combining increasing numbers sensors. Project Tango Google’ Advanced Technology And Projects Group (ATAP) [Google Advanced Technology And Projects 2014] trend. Project Tango generation mobile device depth sensor, motion tracking camera 9-axis IMU enabling calculate position orientation space, simultaneously scanning building 3D map environment. A recent research project team group announced Project Soli [Google Advanced Technology And Projects 2015]. Project Soli radar track hand finger movements -millimetric precision high speed, enabling natural, intuitive interfaces small wearable devices.\nThese examples kinds devices research. But devices share common problem: extracting meaningful information data gesture recognition challenging task. Currently, machine learning popular successful research area field.\nGestural Interaction ( Gesture Recognition) AEHCI\nGestural interaction ( gesture recognition) broad field. This section focus applications AEHCI, context machine learning. Wider surveys found [Mitra Acharya 2007][Gillian 2011] [LaViola Jr. 2013].\nArtificial Neural Networks (ANN) AEHCI map -dimensional input vectors -dimensional output vectors learned -linear function, allowing control complex parameter-sets simultaneously. This regression tasks, manipulating continuous parameters generative visual sonic model. They equally successful classification tasks, recognising gestures triggering desired visual sonic outputs. In 1992 Michael Lee al ANNs inside MAX/MSP musical programming environment investigate adaptive user interfaces realtime musical performance [Lee al. 1992]. They successfully recognise gestures number devices including radio baton continuous space controller. In 1993, Sidney Fels Geoffrey Hinton ANNs map hand movements captured data-glove, speech synthesiser [Fels Hinton 1993]. They achieved realtime results vocabulary 203 gestures--words demonstrating potential neural networks adaptive interfaces. Now, open-source implementations, integrated Rebecca Fiebrink’ Wekinator Nick Gillian’ GRTGui, ANNs widely creative gestural interaction.\nMany machine learning techniques gesture recognition, specific cases. These include K-Nearest Neighbour, Gaussian Mixture Models, Random Forests, Adaptive Naïve Bayes Classifiers Support Vector Machines classify static data; Dynamic Time Warping Hidden Markov Models classify temporal gestures; Linear Regression, Logistic Regression Multivariate Linear Regression real-valued outputs opposed classifying input. A survey machine learning techniques applications musical gesture recognition found [Caramiaux Tanaka 2013].\nAs mentioned previously, artistic, performative context, detecting subtle variations gestures vital conveying expressivity. In [Bevilacqua Muller 2005][Bevilacqua al. 2009], Bevilacqua al design continuous gesture followers temporal gesture recognition realtime gesture performed. This algorithm returns time progression information likelihood, enabling performers alter speed accuracy gesture control parameters generative model.\nIn [Caramiaux al. 2014][Caramiaux 2015] Caramiaux al develop systems classification gestures, characterise qualities gesture’ execution. They computational adaptive models identifying temporal, geometric dynamic variations trained gesture. Returning information realtime performer executing gestures, enables performer map variations parameters time-stretching samples, modulations, volume custom synth parameters.\nIn [Kiefer 2014] Kiefer investigates Echo State (Recurrent Neural) Networks (ESN) mapping tools, learn sequences input gestures, -linearly map multi-parameter output sequences. The research concludes ESNs demonstrate good potential pattern classification, multi-parametric control, explorative nonlinear mapping, room improvement produce accurate results cases.\nInteractive Machine Learning (IML)\nAs discussed , ML successful technique pattern gesture recognition. However machine learning difficult technical knowledge time required building classifiers setting signal processing pipeline [Fails al. 2003].\nInteractive Machine Learning (IML) field process machine learning, lens human computer interaction research [Fiebrink 2011].\nWhile ML brings huge advancements fields data analysis pattern recognition, IML seeks improve ML systems . Particularly, expanding user-base dedicated computer scientists closely related disciplines, wider audience. One ways made Graphic User Interface (GUI) front end ML backend, data streamed live ML backend. The training predictions performed realtime, writing code making perfect choice performance AEHCI.\nRebecca Fiebrink al’ previously mentioned Wekinator software released 2009 IML system aimed musical performance [Fiebrink al. 2009]. Using GUI, users setup, train modify parameters ANN. The software applications — existing music software, visual software, custom generative software — stream data Wekinator UDP-based protocol commonly inter-app inter-device communications called Open Sound Control (OSC) [Wright Freed 1997]. As Wekinator receives data, runs machine learning model streams back predictions realtime. Wekinator number built- sensor input feature extraction capabilities edge detection webcam. Using tool, artists, musicians, dancers, performers researchers fields train map gestures arbitrary outputs, notes, effects, images sounds; programming computer vision software.\nNick Gillian’ Gesture Recognition Toolkit (GRT) 2011 [Gillian 2011] similar functionality emphasis signal processing / gesture recognition pipeline. It lacks built- input functionality webcam microphone inputs, number built- pre-processing, feature extraction post-processing algorithms. Examples Fast Fourier Transform, Principal Component Analysis, filters, derivatives, dead zones . In addition open-source application, underlying codebase released C++ framework allowing integrated bespoke applications.\nRecently NVIDIA released similar GUI based application — Deep Learning GPU Training System (DIGITS) — allowing researchers deep learning similar interactive fashion [NVIDIA 2015]. The software popular open-source deep learning framework Caffe [Jia al. 2014], designed full advantage GPU acceleration, scaling automatically multi-GPU systems.\nIn 2015, research I required similar Multi-Model IML system. One I dynamically create train models leaving existing models intact. I needed access multiple models simultaneously, feeding model inputs receiving predictions. For I developed msaOscML [Akten 2015a]. msaOscML Multi-Model Interactive Machine Learning tool. It inspired similar Rebecca Fiebrink’ Wekinator Nick Gillian’ GRT. However, tools aimed -technical audience, user- friendly Graphical User Interface (GUI), msaOscML aimed creative developers add machine learning capabilities creative software suite, focus -running installations performances minimise hands- operation software. For reason msaOscML GUI. It runs background server console status provide visual feedback user desired. Similar Wekinator GRT, interacted (input output) Open Sound Control (OSC) protocol (Wright & Freed, 1997). The main purpose msaOscML, difference Wekinator GRT, train predict multiple independent models simultaneously, create, save load multiple models directly OSC interface requiring human operator point. This host application (.. software Max MSP, Ableton Live, custom software generating sound visuals) control multiple models directly, system unmanned long performance installation. Cross platform written C++, msaOscML abstraction layer ML implementation, I refer Machine Learning Implementation Abstraction Layer (MLIAL). This machine learning libraries plug backend minimal MLIAL wrapper. Currently MLIAL wrappers Gillian’ GRT framework, Steffen Nissen’ Fast Artificial Neural Network Library (FANN) (Nissen, 2003). msaOscML written R&D interactive dance project called Pattern Recognition (Akten, 2015c).\nTools enable technical -technical users quickly setup, train test models gesture recognition gestural interaction. Without writing code, users start streaming input data sensors, receive predictions application choice, enabling gesturally create, manipulate perform audio-visual content realtime. An Fiebrink’ Wekinator band 000000Swan’ audio-visual shows gesturally driven Microsoft Kinect commercially sensor bow [Schedel al. 2011]. In addition, applied contexts workshops people learning physical disabilities [Katan al. 2015].\nConclusions\nThe field algorithmically generating images sound rich -established field. Expressive interaction — music — -established advanced techniques emerging field maturing. Deep learning revolutionary revival recent developments. The gaming, entertainment media industries converging generation multi-modal interaction virtual, augmented mixed reality technologies mainstream.\nWithin context, unexplored territories lot artistic potential, intersections trends. These include ways generating content deep learning, realtime interactively. Also applications expressive interactions generation content, generation consumer devices set mixed reality environments.\nBibliography\nAKTEN, M. 2015a. msaOscML. https://github./memo/msaOscML.\nAKTEN, M. 2015b. ‘Pattern Recognition’ Dance Performance. http://www.memo.tv/pattern-recognitionwip/.\nAKTEN, M., STEEL, B., MCNICHOLAS, R., ET AL. 2011. Sony PlayStation VideoStore Mapping. .\nAL-RIFAIE, M.M. AND BISHOP, J.M. 2015. Weak Strong Computational Creativity. In: Computational Creativity Research: Towards Creative Machines. 0–14.\nALVES, B. 2005. Digital Harmony Sound Light. Computer Music Journal 29, 4, 45–54.\nBENGIO, Y., COURVILLE, A., AND VINCENT, P. 2013. Representation Learning: A Review New Perspectives. Tpami 1993, 1–30.\nBEVILACQUA, F. AND MULLER, R. 2005. A gesture follower performing arts. Proceedings International Gesture …, 3–4.\nBEVILACQUA, F., ZAMBORLIN, B., SYPNIEWSKI, A., SCHNELL, N., GUÉDY, F., AND RASAMIMANANA, N. 2009. Continuous realtime gesture recognition. Lecture Notes Computer Science (including subseries Lecture Notes Artificial Intelligence Lecture Notes Bioinformatics) 5934 LNAI, 73–84.\nBOULANGER-LEWANDOWSKI, N., VINCENT, P., AND BENGIO, Y. 2012. Modeling Temporal Dependencies HighDimensional Sequences: Application Polyphonic Music Generation Transcription. Proceedings 29th International Conference Machine Learning (ICML-12) Cd, 1159–1166.\nBROWNE, C. AND POWLEY, E. 2012. A survey monte carlo tree search methods. Intelligence AI 4, 1, 1–49.\nCADOZ, C. AND WANDERLEY, M. 2000. Gesture-music. Trends gestural control music, 71–94.\nCAMURRI, A., MAZZARINO, B., RICCHETTI, M., TIMMERS, R., AND VOLPE, G. 2004. Multimodal analysis expressive gesture music dance performances. In: Gesture-based {C}ommunication {H}uman-{C}omputer {I}nteraction, {LNAI} 2915. 20–39.\nCARAMIAUX, B. 2015. Motion Modeling Expressive Interaction A Design Proposal Bayesian Adaptive Systems. International Workshop Movement Computing (MOCO), IRCAM.\nCARAMIAUX, B., BEVILACQUA, F., AND SCHNELL, N. 2009. Towards gesture-sound cross-modal analysis. Lecture Notes Computer Science (including subseries Lecture Notes Artificial Intelligence Lecture Notes Bioinformatics) 5934 LNAI, 158–170.\nCARAMIAUX, B., DONNARUMMA, M., AND TANAKA, A. 2015. Understanding Gesture Expressivity Muscle Sensing. ACM Transactions Computer-Human Interaction 0, 0, 1–27.\nRealtime image & sound synthesis & expressive manipulation DL & RL responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\n21\nCARAMIAUX, B., MONTECCHIO, N., TANAKA, A., AND BEVILACQUA, F. 2014. Adaptive Gesture Recognition Variation Estimation Interactive Systems. ACM Transactions Interactive Intelligent Systems (TiiS) (In Press) V, 212.\nCARAMIAUX, B. AND TANAKA, A. 2013. Machine Learning Musical Gestures. Proceedings International Conference New Interfaces Musical Expression, 513–518.\nCASSELL, J. AND MCNEILL, D. 1991. Gesture Poetics Prose. Poetics Today 12, 3, 375–404.\nCAVALLO, F., PEASE, A., GOW, J., AND COLTON, S. 2013. Using Theory Formation Techniques Invention Fictional Concepts. 176–183.\nCIRESAN, D., MEIER, U., AND MASCI, J. 2011. Flexible, high performance convolutional neural networks image classification. International Joint Conference Artificial Intelligence, 1237–1242.\nCOHEN, H. 1973. Parallel perception: notes problem machine-generated art. Computer Studies, 1–10.\nCOHEN, H. 1994. The Further Exploits Aaron, Painter. .\nCOHEN, H. 2006. AARON, Colorist: Expert System Expert. .\nCOLLOBERT, R., WESTON, J., BOTTOU, L., KARLEN, M., KAVUKCUOGLU, K., AND KUKSA, P. 2011. Natural language processing () scratch. The Journal Machine Learning Research 1, 12, 2493–2537.\nCOLTON, S., GOODWIN, J., AND VEALE, T. 2012. Full-FACE Poetry Generation. Proceedings Third International Conference Computational Creativity (ICCC’12), 95–102.\nCOLTON, S., PEASE, A., AND CHARNLEY, J. 2011. Computational creativity theory: The FACE IDEA descriptive models. Proceedings Second International Conference Computational Creativity, 90–95.\nCOLTON, S. AND WIGGINS, G. A. 2012. Computational creativity: The final frontier? Frontiers Artificial Intelligence Applications 242, 21–26.\nCOOK, M., COLTON, S., AND GOW, J. 2014. Automating Game Design In Three Dimensions. AISB Symposium AI Games, 3–6.\nCOPE, D.H. 2010. Recombinant music composition algorithm method . .\nCOUPRIE, C., NAJMAN, L., AND LECUN, Y. 2013. Learning Hierarchical Features Scene Labeling. Pattern Analysis Machine Intelligence, IEEE Transactions 35, 8, 1915–1929.\nCOWIE, R., DOUGLAS-COWIE, E., TSAPATSOULIS, N., ET AL. 2001. Emotion recognition human-computer interaction. Signal Processing Magazine, IEEE 18, 1, 32–80.\nRealtime image & sound synthesis & expressive manipulation DL & RL responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\n22\nCRUZ-NEIRA, C., SANDIN, D., AND DEFANTI, T. 1993. Surround-screen projection-based virtual reality: design implementation CAVE. … 20Th Annual Conference …, 135–142.\nDENG, J.D.J., DONG, W.D.W., SOCHER, R., LI, L.-J.L.L.-J., LI, K.L.K., AND FEI-FEI, L.F.-F.L. 2009. ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference Computer Vision Pattern Recognition, 2–9.\nDENG, L., HINTON, G., AND KINGSBURY, B. 2013. New Types Deep Neural Network Learning Speech Recognition Related Applications : Overview. 8599–8603.\nDOSOVITSKIY, A. AND BROX, T. 2015. Inverting Convolutional Networks Convolutional Networks. 1–15.\nDOURISH, P. 2001. Where Action Is: The Foundations Embodied Interaction. Where action foundations embodied interaction 36, 233. http://books.google./books?id=DCIy2zxrCqcC&pgis=1.\nDRAVES, S. 2005. The Electric Sheep screen-saver: A case study aesthetic evolution. Proc. EvoMUSART, 458–467.\nECK, D. AND SCHMIDHUBER, J. 2002. A music composition lstm recurrent neural networks. Istituto Dalle Molle Di Studi Sull Intelligenza ….\nERHAN, D., BENGIO, Y., COURVILLE, A., AND VINCENT, P. 2009. Visualizing higher-layer features deep network. Bernoulli 1341, 1–13.\nFAILS, J.A., OLSEN, JR., D.R., AND OLSEN, D.R. 2003. Interactive Machine Learning. Proceedings 8th International Conference Intelligent User Interfaces, ACM, 39–45.\nFELS, S.S. AND HINTON, G.E. 1993. Glove-talk: neural network interface data-glove speech synthesizer. IEEE Transactions Neural Networks 4, 1, 2–8.\nFIEBRINK, R., TRUEMAN, D., AND COOK, P.R. 2009. A metainstrument interactive, --fly machine learning. Proc. NIME 2, 3.\nFIEBRINK, R.A. 2011. Real-time Human Interaction Supervised Learning Algorithms Music Composition Performance. Imagine January, 376.\nGATYS, L. A., ECKER, A.S., AND BETHGE, M. 2015. A Neural Algorithm Artistic Style. 3–7.\nGILLIAN, N.E. 2011. Gesture Recognition Musician Computer Interaction. Social Sciences March.\nGOOGLE ADVANCED TECHNOLOGY AND PROJECTS. 2014. Project Tango. https://www.google./atap/projecttango/.\nGOOGLE ADVANCED TECHNOLOGY AND PROJECTS. 2015. Project Soli. https://www.google./atap/projectsoli/.\nRealtime image & sound synthesis & expressive manipulation DL & RL responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\n23\nGRIERSON, M. 2005. Audiovisual composition. http://www.strangeloop..uk/Dr. M.Grierson — Audiovisual Composition Thesis.pdf.\nGUO, X., SINGH, S., LEE, H., LEWIS, R., AND WANG, X. 2014. Deep Learning Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning. Advances Neural Information Processing Systems (NIPS) 27 2600, 3338–3346.\nGUZELLA, T.S. AND CAMINHAS, W.M. 2009. A review machine learning approaches Spam filtering. Expert Systems Applications 36, 7, 10206–10222.\nHINTON, G., DENG, L., YU, D., ET AL. 2012. Deep Neural Networks Acoustic Modeling Speech Recognition. Ieee Signal Processing Magazine November, 82–97.\nHOCHREITER, S. AND SCHMIDHUBER, J. 1997. Long short-term memory. Neural computation 9, 8, 1735–80.\nILYA SUTSKEVER, ORIOL VINYALS, Q.V. LE. 2014. Sequence Sequence Learning Neural Networks. Nips, 1–9.\nJIA, Y., SHELHAMER, E., DONAHUE, J., ET AL. 2014. Caffe : Convolutional Architecture Fast Feature Embedding. arXiv preprint arXiv:1408.5093.\nJONES, B., SHAPIRA, L., SODHI, R., ET AL. 2014. RoomAlive: Magical Experiences Enabled Scalable, Adaptive Projector-camera Units. Proceedings 27th annual ACM symposium User interface software technology — UIST ’14, 637–644.\nJONES, B.R., BENKO, H., OFEK, E., AND WILSON, A.D. 2013. IllumiRoom: peripheral projected illusions interactive experiences. Proceedings SIGCHI Conference Human Factors Computing Systems — CHI ’13, 869.\nKARPATHY, A. 2015a. char-rnn. https://github./karpathy/char-rnn.\nKARPATHY, A. 2015b. The Unreasonable Effectiveness Recurrent Neural Networks. .\nKARPATHY, ANDREJ, L.F.-F. 2015. Deep Visual-Semantic Alignments Generating Image Descriptions. Cvpr.\nKATAN, S., GRIERSON, M., AND FIEBRINK, R. 2015. Using Interactive Machine Learning Support Interface Development Through Workshops Disabled People. CHI ’15 Proceedings 33rd Annual ACM Conference Human Factors Computing Systems.\nKIEFER, C. 2014. Musical Instrument Mapping Design Echo State Networks. Proceedings International Conference New Interfaces Musical Expression, 293–298.\nKOHLER, E., KEYSERS, C., UMILTÀ, M.A., FOGASSI, L., GALLESE, V., AND RIZZOLATTI, G. 2002. Hearing sounds, understanding actions: action representation mirror neurons. Science (New York, N.Y.) 297, 5582, 846–848.\nRealtime image & sound synthesis & expressive manipulation DL & RL responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\n24\nKRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G.E. 2012. ImageNet Classification Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems, 1–9.\nKRUEGER, M.W., GIONFRIDDO, T., AND HINRICHSEN, K. 1985. VIDEOPLACE — - artificial reality. ACM SIGCHI Bulletin 16, 4, 35–40.\nKYPRIANIDIS, J.E., COLLOMOSSE, J., WANG, T., AND ISENBERG, T. 2013. State ’Art: A taxonomy artistic stylization techniques images video. IEEE Transactions Visualization Computer Graphics 19, 5, 866–885.\nLAVIOLA JR., J.J. 2013. 3D Gestural Interaction: The State Field. ISRN Artificial Intelligence 2013 2013, 2, 1–18.\nLE, Q. V., RANZATO, M.A., MONGA, R., ET AL. 2011. Building high-level features large scale unsupervised learning. International Conference Machine Learning, 38115.\nLEAPMOTION. LeapMotion. https://www.leapmotion./.\nLECUN, Y. 2012. Learning invariant feature hierarchies. Lecture Notes Computer Science (including subseries Lecture Notes Artificial Intelligence Lecture Notes Bioinformatics) 7583 LNCS, PART 1, 496–505.\nLECUN, Y. 2014. The Unreasonable Effectiveness Deep Learning. Facebook AI Research & Center Data Science, NYU.\nLECUN, Y. AND BENGIO, Y. 1995. Convolutional networks images, speech, time series. The handbook brain theory neural networks 3361, 255–258.\nLECUN, Y., BOSER, B., DENKER, J.S., ET AL. 1989. Backpropagation Applied Handwritten Zip Code Recognition. Neural Computation 1, 541–551.\nLECUN, Y., BOTTOU, L., BENGIO, Y., AND HAFFNER, P. 1998. Gradient-based learning applied document recognition. Proceedings IEEE 86, 11, 2278–2323.\nLECUN, Y., CORTES, C., AND BURGES, C.J.C. The MNIST Database. http://yann.lecun./exdb/mnist/index.html.\nLECUN, Y., HUANG, F.J.H.F.J., AND BOTTOU, L. 2004. Learning methods generic object recognition invariance pose lighting. Proceedings 2004 IEEE Computer Society Conference Computer Vision Pattern Recognition, 2004. CVPR 2004. 2.\nLECUN, Y., JACKEL, L., BOTTOU, L., ET AL. 1995. Comparison learning algorithms handwritten digit recognition. International Conference artificial neural networks, 53–60.\nLEE, M., FREED, A., AND WESSEL, D. 1992. Neural networks simultaneous classification parameter estimation musical instrument control. Proceedings SPIE 1706, 244–255.\nRealtime image & sound synthesis & expressive manipulation DL & RL responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\n25\nLEMAN, M. 2007. Embodied Music Cognition Mediation Technology. .\nLEVIN, G. 2000. Painterly Interfaces Audiovisual Performance. Media, 1–151.\nMAHENDRAN, A. AND VEDALDI, A. 2014. Understanding Deep Image Representations Inverting Them. .\nMCCORMACK, J. AND D’INVERNO, M. 2012. Computers Creativity: The Road Ahead. Computers Creativity, 421–424.\nMCCORMACK, J. AND D’INVERNO, M. 2014. On Future Computers Creativity. .\nMCNEILL, D. AND LEVY, E. 1980. Conceptual representations language activity gesture. .\nMETZINGER, T. AND GALLESE, V. 2003. The emergence shared action ontology: Building blocks theory. Consciousness Cognition, 549–571.\nMISTRY, P. AND MAES, P. 2009. SixthSense: wearable gestural interface. ACM SIGGRAPH ASIA 2009 Sketches, ACM.\nMITAL, P.K., GRIERSON, M., AND SMITH, T.J. 2013. Corpus-based visual synthesis. Proceedings ACM Symposium Applied Perception — SAP ’13 July, 51–58.\nMITCHELL, T.M. 1997. Machine Learning. McGraw Hill.\nMITRA, S. AND ACHARYA, T. 2007. Gesture Recognition : A Survey. IEEE Transactions On Systems, Man, And Cybernetics — Part C: Applications And Reviews 37, 3, 311–324.\nMNIH, V., HEESS, N., GRAVES, A., AND KAVUKCUOGLU, K. 2014. Recurrent Models Visual Attention. Nips, 1– 12.\nMNIH, V., KAVUKCUOGLU, K., SILVER, D., ET AL. 2013. Playing Atari Deep Reinforcement Learning. arXiv preprint arXiv: …, 1–9.\nMORDVINTSEV, A., OLAH, C., AND TYKA, M. 2015. Deepdream inceptionism. http://googleresearch.blogspot.ch/2015/06/inceptionism--deeper--neural.html.\nNG, A. 2013. Machine Learning AI Brain Simulations. Stanford University.\nNGUYEN, A, YOSINSKI, J., AND CLUNE, J. 2015. Deep Neural Networks Easily Fooled: High Confidence Predictions Unrecognizable Images. Cvpr 2015.\nNISSEN, S. 2003. Fast Artificial Neural Network Library. http://leenissen.dk/fann/wp/.\nNVIDIA. 2015. Deep Learning GPU Training System (DIGITS). https://developer.nvidia./digits/.\nRAINA, R., MADHAVAN, A., AND NG, A.Y. 2009. Large-scale deep unsupervised learning graphics processors. Icml 9, 873–880.\nRealtime image & sound synthesis & expressive manipulation DL & RL responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\n26\nROKEBY, D. 1986. Very Nervous System. .\nSCHEDEL, M., FIEBRINK, R., AND PERRY, P. 2011. Wekinating 000000Swan : Using Machine Learning Create Control Complex Artistic Systems. Proceedings International Conference New Interfaces Musical Expression June, 453–456.\nSCHMIDHUBER, J. 2014. Deep Learning Neural Networks: An Overview. arXiv preprint arXiv:1404.7828, 1–66.\nSEARLE, J.R. 1980. Minds, Brains, Programs. Behavioral Brain Sciences 3, 1–19.\nSIMONYAN, K., VEDALDI, A., AND ZISSERMAN, A. 2013. Deep Inside Convolutional Networks: Visualising Image Classification Models Saliency Maps. arXiv preprint arXiv:1312.6034, 1–8.\nSIMS, K. 1994. Evolving virtual creatures. Siggraph ’94 SIGGRAPH ’, July, 15–22.\nSTURM, B. 2015. Recurrent Neural Networks Folk Music Generation. https://highnoongmt.wordpress./2015/05/22/lisls-stis-recurrent-neural-networks--folkmusic-generation.\nSZEGEDY, C., LIU, W., JIA, Y., ET AL. 2014. Going Deeper Convolutions. arXiv preprint arXiv:1409.4842, 1–12.\nTHRUN, S., MONTEMERLO, M., DAHLKAMP, H., ET AL. 2006. Stanley: The Robot That Won DARPA Grand Challenge. Journal Field Robotics 23, 9, 661–692.\nTODD, S. AND LATHAM, W. 1992. Evolutionary art computers. Academic Press, Inc.\nTURING, A. 1948. Intelligent Machinery. .\nTURING, A. 1950. Computing Machinery Intelligence. Mind 59, 433–460.\nWRIGHT, M. AND FREED, A. 1997. Open Sound Control: A protocol communicating sound synthesizers. Proceedings International Computer Music Conference (ICMC).\nZENG, Z., PANTIC, M., ROISMAN, G.I., AND HUANG, T.S. 2009. A survey affect recognition methods: Audio, visual, spontaneous expressions. IEEE Transactions Pattern Analysis Machine Intelligence 31, 1, 39–58.',
       attributes: {} },
     Document {
       text:
        'The field machine learning witnessing golden era deep learning slowly leader domain. Deep learning multiple layers represent abstractions data build computational models. Some key enabler deep learning algorithms generative adversarial networks, convolutional neural networks, model transfers completely changed perception information processing. However, exists aperture understanding tremendously fast-paced domain, previously represented multiscope perspective. The lack core understanding renders powerful methods black-box machines inhibit development fundamental level. Moreover, deep learning repeatedly perceived silver bullet stumbling blocks machine learning, truth. This article presents comprehensive review historical recent state---art approaches visual, audio, text processing; social network analysis; natural language processing, -depth analysis pivoting groundbreaking advances deep learning applications. It undertaken review issues faced deep learning unsupervised learning, black-box models, online learning illustrate challenges transformed prolific future research avenues.',
       attributes: {} } ] }
